# KerasANN
Basic regression model ANN built using Keras

![alt text](https://github.com/VBukowsky81/KerasANNBuild/blob/main/KerasANNPic.jpg)

Hello there, good reader!

I wanted to present this great Keras-built ANN example, for demonstration. Artificial neural networks, machine learning and data science - so, so much can be said about these subject matters. It is truly a revolution, not only in programming, but for us as humanity. This discovery/technology/idea will lead us to a race of robots, in not so distant future. We have unlocked secret of creative intelligence - one of the greatest discoveries we have ever made as species. 

We now have a good understanding of how we think, and what's most important - we can recreate some of what we do, as advanced intelligent species.

But let's look at this example.

So this is a great basic regression Keras template I use myself. Keras is a just fantastic library for easy to use(and build) ANN's. PyTorch is obviously king of enterprise neural network programming and data science right now(I mean, Tesla, Facebook, etc - almost everybody uses PyTorch for enterprise scale). But Keras shines where it comes to practicing, visualizing, demonstrating, and smaller-sized neural networks - Keras is perfect for learning, and practicing, etc. 

But I will add PyTorch examples, later on, of course.

Anyway, this has all the basic parts - data preprocessing, setting up batch sizes, variables, testing, etc. Building ANN's in Keras is just so clean and neat. Everything is in one line - layer type, number of nodes, weights initializer, activation function, etc. PyTorch is significantly different, but for a good reasons.

So 100 epochs run, typical simple linear regression task - predicting future data. In this case, we are predicting banking clients turnaround. Typical data science project.

Anyway, next I will demonstrate CNN's and how they work. Then I will go into RNN's/LSTMs(although, of course they are outdated, long ago, but just for demonstration). Next I will build Attention-based model, and dabble into Transformers.

I am a passionate, passionate fan of this field, and I devour any information I find on this. 

I can go on and on and on about this - optimization algorithms, and their advantages/disadvantages, backpropagation and gradient calculation math, ideal batch sizes and NN setups and configurations, for particular goals. Etc, etc.

GAN's, GPT-3(and other amazing language models), i-GPT, GPT-f, CLIP, Reinforcment Learning/Q-learning, Markov's equations. Deeper math and philosical implications of this incredible field. Like I said - I can go on and on for hours.

I am looking to apply myself somewhere in Data Science.
